{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "\n",
    "def pdf_to_text(pdf_path, txt_path):\n",
    "    try:\n",
    "        # Open the PDF file\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            # Create or open the text file to write the content\n",
    "            with open(txt_path, 'w', encoding='utf-8') as txt_file:\n",
    "                # Iterate through all pages\n",
    "                for page_num, page in enumerate(pdf.pages):\n",
    "                    # Extract text from each page\n",
    "                    text = page.extract_text()\n",
    "                    \n",
    "                    # Write the text to the file if it exists\n",
    "                    if text:\n",
    "                        txt_file.write(f'--- Page {page_num + 1} ---\\n')\n",
    "                        txt_file.write(text)\n",
    "                        txt_file.write('\\n')  # Add a new line after each page\n",
    "                \n",
    "        print(f\"Successfully converted '{pdf_path}' to '{txt_path}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "# Example usage:\n",
    "pdf_path = '../../data/static/intl_mil_ops_in_21st_cent.pdf'  # Replace with your PDF file path\n",
    "txt_path = 'output.txt'   # Replace with your desired output text file path\n",
    "pdf_to_text(pdf_path, txt_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments\n",
    "\n",
    "# Save the fine-tuned model and tokenizer\n",
    "model.save_pretrained(\"./retrained_gpt2_model\")\n",
    "tokenizer.save_pretrained(\"./retrained_gpt2_tokenizer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set padding token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load and preprocess your dataset\n",
    "dataset = load_dataset('text', data_files={'train': '../../data/static/artofwar.txt'})\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    tokenized = tokenizer(examples['text'], truncation=True, padding='max_length', max_length=512)\n",
    "    tokenized['labels'] = tokenized['input_ids']\n",
    "    return tokenized\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=2,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    save_steps=10_000,\n",
    "    output_dir='./results'\n",
    ")\n",
    "\n",
    "# Create Trainer instance\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset['train']\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/homebrew/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "# Load and preprocess your dataset\n",
    "dataset = load_dataset('text', data_files={'train': 'output.txt'})\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    tokenized = tokenizer(examples['text'], truncation=True, padding='max_length', max_length=512)\n",
    "    tokenized['labels'] = tokenized['input_ids']\n",
    "    return tokenized\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=2,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    save_steps=10_000,\n",
    "    output_dir='./results'\n",
    ")\n",
    "\n",
    "# Create Trainer instance\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset['train']\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model and tokenizer\n",
    "model.save_pretrained(\"./retrained_gpt2_model\")\n",
    "tokenizer.save_pretrained(\"./retrained_gpt2_tokenizer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fine-tuned model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"./retrained_gpt2_tokenizer\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"./retrained_gpt2_model\")\n",
    "\n",
    "# Add a PAD token if needed\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define mission details\n",
    "available_vehicles = \"foot, land vehicle, helicopter, boat\"\n",
    "starting_location = \"lat: 34.0522, long: -118.2437\"  # Example coordinates for Los Angeles\n",
    "target_location = \"lat: 36.1699, long: -115.1398\"    # Example coordinates for Las Vegas\n",
    "total_personnel = 10\n",
    "target_time_on_objective = \"2 hours\"\n",
    "strategy = \"stealth\"\n",
    "objective = \"infiltrate target\"\n",
    "expected_resistance = \"high\"\n",
    "\n",
    "prompt = (\n",
    "    f\"Generate a detailed mission plan based on the following details:\\n\"\n",
    "    f\"- Available vehicles: {available_vehicles}\\n\"\n",
    "    f\"- Starting location: {starting_location}\\n\"\n",
    "    f\"- Target location: {target_location}\\n\"\n",
    "    f\"- Total personnel: {total_personnel}\\n\"\n",
    "    f\"- Target time on objective: {target_time_on_objective}\\n\"\n",
    "    f\"- Strategy: {strategy}\\n\"\n",
    "    f\"- Objective: {objective}\\n\"\n",
    "    f\"- Expected resistance: {expected_resistance}\\n\"\n",
    "    \"\\nplan:\"\n",
    ")\n",
    "\n",
    "# Tokenize the input text and set attention mask\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# Generate output with more detailed configuration\n",
    "outputs = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],\n",
    "    max_new_tokens=300,               # Increase the token length\n",
    "    do_sample=True,                   # Enable sampling\n",
    "    temperature=0.65,                 # Lower temperature for more coherent text\n",
    "    top_p=0.85,                       # Nucleus sampling (slightly lower for more focus)\n",
    "    repetition_penalty=1.2,           # Add a penalty to discourage repetition\n",
    ")\n",
    "\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
